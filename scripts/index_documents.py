import os
import glob
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.search.documents import SearchClient
from openai import AzureOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tiktoken

# .env dosyasÄ±nÄ± yÃ¼kle (API anahtarlarÄ± iÃ§in)
load_dotenv(override=True)

# -------------------------------------------------------------------------
# KONFIGÃœRASYON
# -------------------------------------------------------------------------

# 1. Document Intelligence (Yeni OluÅŸturduÄŸunuz)
DOC_INTEL_ENDPOINT = os.getenv("AZURE_FORM_RECOGNIZER_ENDPOINT")
DOC_INTEL_KEY = os.getenv("AZURE_FORM_RECOGNIZER_KEY")

# 2. Azure OpenAI (Embedding & Chat)
OPENAI_ENDPOINT = "https://vectorizervascularr.cognitiveservices.azure.com"
OPENAI_KEY = os.getenv("AZURE_OPENAI_API_KEY") # 84ga... olan
EMBEDDING_DEPLOYMENT = "text-embedding-3-large-957047"

# 3. Azure AI Search
SEARCH_ENDPOINT = "https://search-rag-prod-3mktjtlo.search.windows.net"
SEARCH_KEY = os.getenv("AZURE_SEARCH_KEY")
INDEX_NAME = "documents-index"

def sanitize_key(text):
    """
    Azure AI Search document key'i iÃ§in geÃ§erli karakterlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    Allowed: letters, digits, underscore (_), dash (-), equal sign (=)
    """
    # Replace invalid characters with underscore
    # Keep only: a-z, A-Z, 0-9, -, _, =
    sanitized = re.sub(r'[^a-zA-Z0-9\-_=]', '_', text)

    # Remove consecutive underscores
    sanitized = re.sub(r'_+', '_', sanitized)

    # Remove leading/trailing underscores
    sanitized = sanitized.strip('_')

    return sanitized

def init_clients():
    """TÃ¼m client'larÄ± baÅŸlat."""
    if not all([DOC_INTEL_ENDPOINT, DOC_INTEL_KEY, OPENAI_KEY, SEARCH_KEY]):
        print("HATA: LÃ¼tfen .env dosyasÄ±nÄ± tÃ¼m anahtarlarla doldurun!")
        return None, None, None

    # Document Intelligence Client
    doc_client = DocumentAnalysisClient(
        endpoint=DOC_INTEL_ENDPOINT, 
        credential=AzureKeyCredential(DOC_INTEL_KEY)
    )

    # OpenAI Client
    openai_client = AzureOpenAI(
        api_key=OPENAI_KEY,
        api_version="2024-12-01-preview",
        azure_endpoint=OPENAI_ENDPOINT
    )

    # Search Client
    search_client = SearchClient(
        endpoint=SEARCH_ENDPOINT,
        index_name=INDEX_NAME,
        credential=AzureKeyCredential(SEARCH_KEY)
    )

    return doc_client, openai_client, search_client

def extract_text_from_pdf(doc_client, file_path):
    """PDF'ten metin Ã§Ä±karÄ±r (TÃ¼m dÃ¶kÃ¼man birleÅŸtirilmiÅŸ)."""
    print(f"ğŸ“„ Okunuyor: {file_path}...")
    with open(file_path, "rb") as f:
        poller = doc_client.begin_analyze_document("prebuilt-read", document=f)
        result = poller.result()

    # TÃ¼m sayfalarÄ± birleÅŸtir (semantic chunking iÃ§in)
    full_text = ""
    page_boundaries = []  # Her sayfanÄ±n baÅŸlangÄ±Ã§ pozisyonunu tut

    for page in result.pages:
        page_start = len(full_text)
        page_boundaries.append({
            "page_num": page.page_number,
            "start_pos": page_start
        })

        # SayfanÄ±n metnini ekle
        page_text = " ".join([line.content for line in page.lines])
        full_text += page_text + "\n\n"  # Sayfa aralarÄ±na boÅŸluk

    print(f"   âœ… {len(result.pages)} sayfa okundu, toplam {len(full_text)} karakter.")
    return full_text, page_boundaries

def create_semantic_chunks(text, page_boundaries):
    """
    Metni semantic chunking ile bÃ¶ler (akademik makaleler iÃ§in optimize edilmiÅŸ).

    Args:
        text: TÃ¼m dÃ¶kÃ¼man metni
        page_boundaries: Her sayfanÄ±n baÅŸlangÄ±Ã§ pozisyonu

    Returns:
        List of chunks with metadata
    """
    # Token counter (OpenAI embedding modeli iÃ§in)
    encoding = tiktoken.encoding_for_model("text-embedding-3-large")

    # Semantic Text Splitter (akademik makaleler iÃ§in optimize)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,           # ~750-800 token (gÃ¼venli limit)
        chunk_overlap=200,          # Context korunmasÄ± iÃ§in overlap
        length_function=lambda t: len(encoding.encode(t)),  # Token bazlÄ±
        separators=[
            "\n\n",                 # Paragraf (en Ã¶nemli)
            "\n",                   # SatÄ±r
            ". ",                   # CÃ¼mle
            " ",                    # Kelime
            ""                      # Karakter (fallback)
        ],
        is_separator_regex=False
    )

    # Chunking yap
    chunks = text_splitter.split_text(text)

    print(f"   ğŸ§© {len(chunks)} semantic chunk oluÅŸturuldu (avg ~{len(text)//len(chunks) if chunks else 0} char/chunk)")

    # Her chunk iÃ§in sayfa numarasÄ±nÄ± bul
    chunks_with_metadata = []
    current_pos = 0

    for i, chunk in enumerate(chunks):
        # Bu chunk hangi sayfada baÅŸlÄ±yor?
        chunk_page = 1
        for boundary in page_boundaries:
            if current_pos >= boundary["start_pos"]:
                chunk_page = boundary["page_num"]

        chunks_with_metadata.append({
            "content": chunk,
            "chunk_id": i + 1,
            "page_num": chunk_page,
            "token_count": len(encoding.encode(chunk))
        })

        # Bir sonraki chunk'Ä±n pozisyonunu tahmin et (overlap dÃ¼ÅŸÃ¼lerek)
        current_pos += len(chunk) - 200  # overlap kadar geri git

    return chunks_with_metadata

def get_indexed_sources(search_client):
    """
    Azure AI Search'te zaten indexlenmiÅŸ PDF'lerin listesini dÃ¶ndÃ¼rÃ¼r.

    Returns:
        set: IndexlenmiÅŸ PDF dosya isimleri (set)
    """
    try:
        # Search'te unique source'larÄ± al
        results = search_client.search(
            search_text="*",
            select="source",
            top=1000  # Max chunk sayÄ±sÄ±
        )

        indexed_sources = set()
        for result in results:
            if "source" in result:
                indexed_sources.add(result["source"])

        return indexed_sources
    except Exception as e:
        print(f"   âš ï¸  Mevcut indexler alÄ±namadÄ±: {e}")
        return set()

def generate_embedding(openai_client, text):
    """Metni vektÃ¶re Ã§evirir."""
    response = openai_client.embeddings.create(
        input=text,
        model=EMBEDDING_DEPLOYMENT
    )
    return response.data[0].embedding

def process_single_pdf(pdf_file, doc_client, openai_client, indexed_sources, force_reindex):
    """
    Tek bir PDF'i iÅŸler (paralel execution iÃ§in).

    Returns:
        tuple: (filename, documents_list, success, error_message)
    """
    filename = os.path.basename(pdf_file)

    try:
        # Skip if already indexed
        if filename in indexed_sources and not force_reindex:
            return (filename, [], True, "skipped")

        print(f"ğŸ“š Ä°ÅŸleniyor: {filename}")

        # 1. PDF'ten Metin Ã‡Ä±kar (Document Intelligence)
        full_text, page_boundaries = extract_text_from_pdf(doc_client, pdf_file)

        if not full_text.strip():
            return (filename, [], False, "DÃ¶kÃ¼man boÅŸ")

        # 2. Semantic Chunking
        chunks = create_semantic_chunks(full_text, page_boundaries)

        # 3. Her Chunk iÃ§in Embedding OluÅŸtur
        documents = []
        print(f"   ğŸ”„ {filename}: {len(chunks)} chunk iÃ§in embedding oluÅŸturuluyor...")

        for chunk in chunks:
            content = chunk["content"]

            # Embedding al
            vector = generate_embedding(openai_client, content)

            # Search DokÃ¼manÄ± YapÄ±sÄ±
            # Sanitize the key to remove invalid characters
            safe_filename = sanitize_key(filename.replace(".pdf", ""))
            doc = {
                "id": f"{safe_filename}-chunk{chunk['chunk_id']}",
                "content": content,
                "title": filename,
                "source": filename,
                "chunk_id": chunk["chunk_id"],
                "content_vector": vector
            }
            documents.append(doc)

            # Rate limiting (daha agresif - paralel olduÄŸu iÃ§in)
            time.sleep(0.5)

        print(f"   âœ… {filename}: {len(documents)} chunk hazÄ±r")
        return (filename, documents, True, None)

    except Exception as e:
        print(f"   âŒ {filename}: Hata - {str(e)}")
        return (filename, [], False, str(e))

def index_files(folder_path="data", force_reindex=False, parallel=False, max_workers=2):
    """
    PDF dosyalarÄ±nÄ± indexler (semantic chunking ile).

    Args:
        folder_path: PDF'lerin bulunduÄŸu klasÃ¶r
        force_reindex: True ise tÃ¼m dosyalarÄ± yeniden indexler
        parallel: True ise paralel processing kullan
        max_workers: Paralel processing iÃ§in worker sayÄ±sÄ± (default: 2)
    """
    doc_client, openai_client, search_client = init_clients()
    if not doc_client:
        return

    pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
    if not pdf_files:
        print(f"ğŸ“‚ '{folder_path}' klasÃ¶rÃ¼nde PDF bulunamadÄ±.")
        return

    # Zaten indexlenmiÅŸ dosyalarÄ± al
    if not force_reindex:
        print(f"ğŸ” Mevcut index kontrol ediliyor...")
        indexed_sources = get_indexed_sources(search_client)
        print(f"   â„¹ï¸  {len(indexed_sources)} dÃ¶kÃ¼man zaten indexlenmiÅŸ")
    else:
        indexed_sources = set()
        print(f"ğŸ”„ Force reindex modu - tÃ¼m dosyalar yeniden iÅŸlenecek")

    # Filter out already indexed files
    files_to_process = [
        f for f in pdf_files
        if force_reindex or os.path.basename(f) not in indexed_sources
    ]

    skipped_count = len(pdf_files) - len(files_to_process)

    if skipped_count > 0:
        print(f"â­ï¸  {skipped_count} dÃ¶kÃ¼man atlanÄ±yor (zaten indexlenmiÅŸ)")

    documents_to_upload = []
    processed_count = 0
    failed_count = 0

    if not files_to_process:
        print(f"\nâœ… HiÃ§ yeni dÃ¶kÃ¼man yok - tÃ¼m PDF'ler zaten indexlenmiÅŸ!")
        print(f"   ğŸ’¡ Yeniden indexlemek iÃ§in: python index_documents.py --force")
        return

    # PARALEL Ä°ÅLEM
    if parallel and len(files_to_process) > 1:
        print(f"\nâš¡ PARALEL MOD: {max_workers} worker ile {len(files_to_process)} PDF iÅŸleniyor...")
        print(f"{'='*60}\n")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_pdf = {
                executor.submit(
                    process_single_pdf,
                    pdf_file,
                    doc_client,
                    openai_client,
                    indexed_sources,
                    force_reindex
                ): pdf_file
                for pdf_file in files_to_process
            }

            # Collect results as they complete
            for future in as_completed(future_to_pdf):
                filename, documents, success, error = future.result()

                if error == "skipped":
                    skipped_count += 1
                elif success:
                    documents_to_upload.extend(documents)
                    processed_count += 1
                else:
                    failed_count += 1
                    print(f"   âš ï¸  {filename} iÅŸlenemedi: {error}")

    # SERI Ä°ÅLEM (Default)
    else:
        if parallel:
            print(f"\nğŸ“ SERI MOD: Tek PDF var, paralel gerek yok")

        print(f"\n{'='*60}")

        for pdf_file in files_to_process:
            filename = os.path.basename(pdf_file)

            print(f"\nğŸ“š Ä°ÅŸleniyor ({processed_count + 1}/{len(files_to_process)}): {filename}")
            print(f"{'='*60}")

            result_filename, documents, success, error = process_single_pdf(
                pdf_file, doc_client, openai_client, indexed_sources, force_reindex
            )

            if error == "skipped":
                skipped_count += 1
            elif success:
                documents_to_upload.extend(documents)
                processed_count += 1
            else:
                failed_count += 1

    # 4. Toplu YÃ¼kleme
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Ä°ÅŸlem Ã–zeti")
    print(f"{'='*60}")
    print(f"   âœ… BaÅŸarÄ±lÄ±: {processed_count} dÃ¶kÃ¼man")
    print(f"   â­ï¸  Atlanan: {skipped_count} dÃ¶kÃ¼man")
    if failed_count > 0:
        print(f"   âŒ BaÅŸarÄ±sÄ±z: {failed_count} dÃ¶kÃ¼man")
    print(f"   ğŸ“¦ Yeni chunk: {len(documents_to_upload)}")

    if documents_to_upload:
        print(f"\n{'='*60}")
        print(f"ğŸš€ {len(documents_to_upload)} chunk Azure AI Search'e yÃ¼kleniyor...")
        print(f"{'='*60}")

        # Batch upload (100'lÃ¼k gruplar halinde)
        batch_size = 100
        for i in range(0, len(documents_to_upload), batch_size):
            batch = documents_to_upload[i:i + batch_size]
            result = search_client.upload_documents(documents=batch)
            print(f"   ğŸ“¦ Batch {i//batch_size + 1}: {len(batch)} chunk yÃ¼klendi")

        print(f"\nâœ… Yeni dÃ¶kÃ¼manlar baÅŸarÄ±yla indexlendi!")
        print(f"   ğŸ“Š Toplam yeni chunk: {len(documents_to_upload)}")
    elif processed_count == 0 and skipped_count > 0:
        print(f"\nâœ… HiÃ§ yeni dÃ¶kÃ¼man yok - tÃ¼m PDF'ler zaten indexlenmiÅŸ!")
        print(f"   ğŸ’¡ Yeniden indexlemek iÃ§in: python index_documents.py --force")

if __name__ == "__main__":
    import sys

    # Parse command line arguments
    force_reindex = "--force" in sys.argv or "-f" in sys.argv
    parallel = "--parallel" in sys.argv or "-p" in sys.argv

    # Get max workers if specified
    max_workers = 2  # default
    for arg in sys.argv:
        if arg.startswith("--workers="):
            try:
                max_workers = int(arg.split("=")[1])
                max_workers = max(1, min(max_workers, 5))  # Limit 1-5
            except:
                pass

    # 'data' klasÃ¶rÃ¼ne PDF atÄ±p Ã§alÄ±ÅŸtÄ±rÄ±n
    if not os.path.exists("data"):
        os.makedirs("data")
        print("ğŸ“ Rehber: 'data' klasÃ¶rÃ¼ oluÅŸturuldu. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya atÄ±n.")
    else:
        print(f"\nğŸš€ PDF Indexing Script")
        print(f"{'='*60}")
        if force_reindex:
            print(f"âš ï¸  FORCE REINDEX MODE: TÃ¼m dosyalar yeniden iÅŸlenecek")
        else:
            print(f"âœ… INCREMENTAL MODE: Sadece yeni dosyalar iÅŸlenecek")

        if parallel:
            print(f"âš¡ PARALLEL MODE: {max_workers} workers")
        else:
            print(f"ğŸ“ SEQUENTIAL MODE")
        print(f"{'='*60}\n")

        index_files("data", force_reindex=force_reindex, parallel=parallel, max_workers=max_workers)

        print(f"\n{'='*60}")
        print(f"âœ¨ Ä°ÅŸlem tamamlandÄ±!")
        print(f"{'='*60}")
