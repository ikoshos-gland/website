import os
import glob
import time
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.search.documents import SearchClient
from openai import AzureOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tiktoken

# .env dosyasÄ±nÄ± yÃ¼kle (API anahtarlarÄ± iÃ§in)
load_dotenv(override=True)

# -------------------------------------------------------------------------
# KONFIGÃœRASYON
# -------------------------------------------------------------------------

# 1. Document Intelligence (Yeni OluÅŸturduÄŸunuz)
DOC_INTEL_ENDPOINT = os.getenv("AZURE_FORM_RECOGNIZER_ENDPOINT")
DOC_INTEL_KEY = os.getenv("AZURE_FORM_RECOGNIZER_KEY")

# 2. Azure OpenAI (Embedding & Chat)
OPENAI_ENDPOINT = "https://vectorizervascularr.cognitiveservices.azure.com"
OPENAI_KEY = os.getenv("AZURE_OPENAI_API_KEY") # 84ga... olan
EMBEDDING_DEPLOYMENT = "text-embedding-3-large-957047"

# 3. Azure AI Search
SEARCH_ENDPOINT = "https://search-rag-prod-3mktjtlo.search.windows.net"
SEARCH_KEY = os.getenv("AZURE_SEARCH_KEY")
INDEX_NAME = "documents-index"

def init_clients():
    """TÃ¼m client'larÄ± baÅŸlat."""
    if not all([DOC_INTEL_ENDPOINT, DOC_INTEL_KEY, OPENAI_KEY, SEARCH_KEY]):
        print("HATA: LÃ¼tfen .env dosyasÄ±nÄ± tÃ¼m anahtarlarla doldurun!")
        return None, None, None

    # Document Intelligence Client
    doc_client = DocumentAnalysisClient(
        endpoint=DOC_INTEL_ENDPOINT, 
        credential=AzureKeyCredential(DOC_INTEL_KEY)
    )

    # OpenAI Client
    openai_client = AzureOpenAI(
        api_key=OPENAI_KEY,
        api_version="2024-12-01-preview",
        azure_endpoint=OPENAI_ENDPOINT
    )

    # Search Client
    search_client = SearchClient(
        endpoint=SEARCH_ENDPOINT,
        index_name=INDEX_NAME,
        credential=AzureKeyCredential(SEARCH_KEY)
    )

    return doc_client, openai_client, search_client

def extract_text_from_pdf(doc_client, file_path):
    """PDF'ten metin Ã§Ä±karÄ±r (TÃ¼m dÃ¶kÃ¼man birleÅŸtirilmiÅŸ)."""
    print(f"ğŸ“„ Okunuyor: {file_path}...")
    with open(file_path, "rb") as f:
        poller = doc_client.begin_analyze_document("prebuilt-read", document=f)
        result = poller.result()

    # TÃ¼m sayfalarÄ± birleÅŸtir (semantic chunking iÃ§in)
    full_text = ""
    page_boundaries = []  # Her sayfanÄ±n baÅŸlangÄ±Ã§ pozisyonunu tut

    for page in result.pages:
        page_start = len(full_text)
        page_boundaries.append({
            "page_num": page.page_number,
            "start_pos": page_start
        })

        # SayfanÄ±n metnini ekle
        page_text = " ".join([line.content for line in page.lines])
        full_text += page_text + "\n\n"  # Sayfa aralarÄ±na boÅŸluk

    print(f"   âœ… {len(result.pages)} sayfa okundu, toplam {len(full_text)} karakter.")
    return full_text, page_boundaries

def create_semantic_chunks(text, page_boundaries):
    """
    Metni semantic chunking ile bÃ¶ler (akademik makaleler iÃ§in optimize edilmiÅŸ).

    Args:
        text: TÃ¼m dÃ¶kÃ¼man metni
        page_boundaries: Her sayfanÄ±n baÅŸlangÄ±Ã§ pozisyonu

    Returns:
        List of chunks with metadata
    """
    # Token counter (OpenAI embedding modeli iÃ§in)
    encoding = tiktoken.encoding_for_model("text-embedding-3-large")

    # Semantic Text Splitter (akademik makaleler iÃ§in optimize)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,           # ~750-800 token (gÃ¼venli limit)
        chunk_overlap=200,          # Context korunmasÄ± iÃ§in overlap
        length_function=lambda t: len(encoding.encode(t)),  # Token bazlÄ±
        separators=[
            "\n\n",                 # Paragraf (en Ã¶nemli)
            "\n",                   # SatÄ±r
            ". ",                   # CÃ¼mle
            " ",                    # Kelime
            ""                      # Karakter (fallback)
        ],
        is_separator_regex=False
    )

    # Chunking yap
    chunks = text_splitter.split_text(text)

    print(f"   ğŸ§© {len(chunks)} semantic chunk oluÅŸturuldu (avg ~{len(text)//len(chunks) if chunks else 0} char/chunk)")

    # Her chunk iÃ§in sayfa numarasÄ±nÄ± bul
    chunks_with_metadata = []
    current_pos = 0

    for i, chunk in enumerate(chunks):
        # Bu chunk hangi sayfada baÅŸlÄ±yor?
        chunk_page = 1
        for boundary in page_boundaries:
            if current_pos >= boundary["start_pos"]:
                chunk_page = boundary["page_num"]

        chunks_with_metadata.append({
            "content": chunk,
            "chunk_id": i + 1,
            "page_num": chunk_page,
            "token_count": len(encoding.encode(chunk))
        })

        # Bir sonraki chunk'Ä±n pozisyonunu tahmin et (overlap dÃ¼ÅŸÃ¼lerek)
        current_pos += len(chunk) - 200  # overlap kadar geri git

    return chunks_with_metadata

def generate_embedding(openai_client, text):
    """Metni vektÃ¶re Ã§evirir."""
    response = openai_client.embeddings.create(
        input=text,
        model=EMBEDDING_DEPLOYMENT
    )
    return response.data[0].embedding

def index_files(folder_path="data"):
    """
    PDF dosyalarÄ±nÄ± indexler (semantic chunking ile).
    """
    doc_client, openai_client, search_client = init_clients()
    if not doc_client:
        return

    pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
    if not pdf_files:
        print(f"ğŸ“‚ '{folder_path}' klasÃ¶rÃ¼nde PDF bulunamadÄ±.")
        return

    documents_to_upload = []

    for pdf_file in pdf_files:
        filename = os.path.basename(pdf_file)
        print(f"\n{'='*60}")
        print(f"ğŸ“š Ä°ÅŸleniyor: {filename}")
        print(f"{'='*60}")

        # 1. PDF'ten Metin Ã‡Ä±kar (Document Intelligence)
        full_text, page_boundaries = extract_text_from_pdf(doc_client, pdf_file)

        if not full_text.strip():
            print(f"   âš ï¸  DÃ¶kÃ¼man boÅŸ, atlanÄ±yor.")
            continue

        # 2. Semantic Chunking
        chunks = create_semantic_chunks(full_text, page_boundaries)

        # 3. Her Chunk iÃ§in Embedding OluÅŸtur
        print(f"   ğŸ”„ Embedding'ler oluÅŸturuluyor...")
        for chunk in chunks:
            content = chunk["content"]

            # Embedding al
            vector = generate_embedding(openai_client, content)

            # Search DokÃ¼manÄ± YapÄ±sÄ±
            doc = {
                "id": f"{filename}-chunk{chunk['chunk_id']}".replace(".", "_").replace(" ", "_"),
                "content": content,
                "title": filename,
                "source": filename,
                "chunk_id": chunk["chunk_id"],
                "content_vector": vector
            }
            documents_to_upload.append(doc)

            print(f"   âœ… Chunk {chunk['chunk_id']}/{len(chunks)} | Page {chunk['page_num']} | {chunk['token_count']} tokens")
            time.sleep(0.3)  # Rate limit korumasÄ±

    # 4. Toplu YÃ¼kleme
    if documents_to_upload:
        print(f"\n{'='*60}")
        print(f"ğŸš€ {len(documents_to_upload)} chunk Azure AI Search'e yÃ¼kleniyor...")
        print(f"{'='*60}")

        # Batch upload (1000'lik gruplar halinde)
        batch_size = 100
        for i in range(0, len(documents_to_upload), batch_size):
            batch = documents_to_upload[i:i + batch_size]
            result = search_client.upload_documents(documents=batch)
            print(f"   ğŸ“¦ Batch {i//batch_size + 1}: {len(batch)} chunk yÃ¼klendi")

        print(f"\nâœ… TÃ¼m dÃ¶kÃ¼manlar baÅŸarÄ±yla indexlendi!")
        print(f"   ğŸ“Š Toplam: {len(documents_to_upload)} semantic chunk")
    else:
        print("âš ï¸  YÃ¼klenecek veri yok.")

if __name__ == "__main__":
    # 'data' klasÃ¶rÃ¼ne PDF atÄ±p Ã§alÄ±ÅŸtÄ±rÄ±n
    if not os.path.exists("data"):
        os.makedirs("data")
        print("Rehber: 'data' klasÃ¶rÃ¼ oluÅŸturuldu. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya atÄ±n.")
    else:
        index_files("data")
