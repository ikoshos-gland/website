import os
import glob
import time
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.search.documents import SearchClient
from openai import AzureOpenAI

# .env dosyasÄ±nÄ± yÃ¼kle (API anahtarlarÄ± iÃ§in)
load_dotenv(override=True)

# -------------------------------------------------------------------------
# KONFIGÃœRASYON
# -------------------------------------------------------------------------

# 1. Document Intelligence (Yeni OluÅŸturduÄŸunuz)
DOC_INTEL_ENDPOINT = os.getenv("AZURE_FORM_RECOGNIZER_ENDPOINT")
DOC_INTEL_KEY = os.getenv("AZURE_FORM_RECOGNIZER_KEY")

# 2. Azure OpenAI (Embedding & Chat)
OPENAI_ENDPOINT = "https://vectorizervascularr.cognitiveservices.azure.com"
OPENAI_KEY = os.getenv("AZURE_OPENAI_API_KEY") # 84ga... olan
EMBEDDING_DEPLOYMENT = "text-embedding-3-large-957047"

# 3. Azure AI Search
SEARCH_ENDPOINT = "https://search-rag-prod-3mktjtlo.search.windows.net"
SEARCH_KEY = os.getenv("AZURE_SEARCH_KEY")
INDEX_NAME = "documents-index"

def init_clients():
    """TÃ¼m client'larÄ± baÅŸlat."""
    if not all([DOC_INTEL_ENDPOINT, DOC_INTEL_KEY, OPENAI_KEY, SEARCH_KEY]):
        print("HATA: LÃ¼tfen .env dosyasÄ±nÄ± tÃ¼m anahtarlarla doldurun!")
        return None, None, None

    # Document Intelligence Client
    doc_client = DocumentAnalysisClient(
        endpoint=DOC_INTEL_ENDPOINT, 
        credential=AzureKeyCredential(DOC_INTEL_KEY)
    )

    # OpenAI Client
    openai_client = AzureOpenAI(
        api_key=OPENAI_KEY,
        api_version="2024-12-01-preview",
        azure_endpoint=OPENAI_ENDPOINT
    )

    # Search Client
    search_client = SearchClient(
        endpoint=SEARCH_ENDPOINT,
        index_name=INDEX_NAME,
        credential=AzureKeyCredential(SEARCH_KEY)
    )

    return doc_client, openai_client, search_client

def extract_text_from_pdf(doc_client, file_path):
    """PDF'ten metin Ã§Ä±karÄ±r (Sayfa sayfa)."""
    print(f"ðŸ“„ Okunuyor: {file_path}...")
    with open(file_path, "rb") as f:
        poller = doc_client.begin_analyze_document("prebuilt-read", document=f)
        result = poller.result()

    pages_text = []
    for page in result.pages:
        # Her sayfanÄ±n metnini birleÅŸtir
        text = " ".join([line.content for line in page.lines])
        pages_text.append({"page_num": page.page_number, "content": text})
    
    print(f"   âœ… {len(pages_text)} sayfa okundu.")
    return pages_text

def generate_embedding(openai_client, text):
    """Metni vektÃ¶re Ã§evirir."""
    # Metni Ã§ok uzunsa burada split etmek gerekebilir (Chunking).
    # Basitlik iÃ§in sayfa bazlÄ± yapÄ±yoruz ama production'da 
    # LangChain TextSplitter kullanmak daha iyidir.
    response = openai_client.embeddings.create(
        input=text,
        model=EMBEDDING_DEPLOYMENT
    )
    return response.data[0].embedding

def index_files(folder_path="data"):
    doc_client, openai_client, search_client = init_clients()
    if not doc_client:
        return

    pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
    if not pdf_files:
        print(f"ðŸ“‚ '{folder_path}' klasÃ¶rÃ¼nde PDF bulunamadÄ±.")
        return

    documents_to_upload = []
    
    for pdf_file in pdf_files:
        filename = os.path.basename(pdf_file)
        
        # 1. Metni Ã‡Ä±kar
        pages = extract_text_from_pdf(doc_client, pdf_file)
        
        # 2. VektÃ¶r OluÅŸtur ve HazÄ±rla
        for page in pages:
            content = page["content"]
            if not content.strip(): 
                continue

            # Embedding al
            vector = generate_embedding(openai_client, content)

            # Search DokÃ¼manÄ± YapÄ±sÄ±
            doc = {
                "id": f"{filename}-{page['page_num']}".replace(".", "_").replace(" ", "_"),
                "content": content,
                "title": filename,
                "source": filename,
                "chunk_id": page["page_num"],
                "content_vector": vector
            }
            documents_to_upload.append(doc)
            print(f"   ðŸ§© VektÃ¶r oluÅŸturuldu: Sayfa {page['page_num']}")
            time.sleep(0.5) # Rate limit korumasÄ±

    # 3. Search'e YÃ¼kle
    if documents_to_upload:
        print(f"ðŸš€ {len(documents_to_upload)} parÃ§a Azure AI Search'e yÃ¼kleniyor...")
        result = search_client.upload_documents(documents=documents_to_upload)
        print("âœ… YÃ¼kleme TamamlandÄ±!")
    else:
        print("YÃ¼klenecek veri yok.")

if __name__ == "__main__":
    # 'data' klasÃ¶rÃ¼ne PDF atÄ±p Ã§alÄ±ÅŸtÄ±rÄ±n
    if not os.path.exists("data"):
        os.makedirs("data")
        print("Rehber: 'data' klasÃ¶rÃ¼ oluÅŸturuldu. LÃ¼tfen PDF dosyalarÄ±nÄ±zÄ± buraya atÄ±n.")
    else:
        index_files("data")
