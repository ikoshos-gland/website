# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üéØ Quick Reference for Claude Code

### When Making Changes:
1. ‚úÖ **Always read files before editing** (use Read tool)
2. ‚úÖ **Test changes locally** when possible
3. ‚úÖ **Follow git workflow** (see Git Workflow section below)
4. ‚úÖ **Update documentation** if architecture changes
5. ‚úÖ **Check security** - never commit secrets

### Key Resources:
- **Function App**: `func-rag-prod-3mktjtlolzx3q`
- **Resource Group**: `rg-rag-prod`
- **Key Vault**: `kv-rag-prod-3mktjtlo`
- **Search Index**: `documents-index`
- **OpenAI Endpoint**: `https://vectorizervascularr.cognitiveservices.azure.com`

---

## Project Overview

This repository contains two distinct systems:

1. **Static Portfolio Website** - React frontend deployed to Azure Static Web Apps
2. **RAG/Agentic Backend** - Python Azure Functions for AI-powered document retrieval with semantic search

---

# Part 1: Static Portfolio Website

A React-based portfolio website featuring interactive 3D elements via Spline. Communicates with the backend via REST API calls only.

## Development Commands

```bash
npm install          # Install dependencies
npm run dev          # Start dev server on port 3000
npm run build        # Production build
npm run preview      # Preview production build
```

## Architecture

### Tech Stack
- React 19 with TypeScript
- Vite 6 for builds
- Tailwind CSS for styling
- Spline for 3D elements (@splinetool/react-spline)
- Lucide React for icons

### Key Files
- `App.tsx` - Orchestrates loading, lazy-loading, and chat widget state
- `components/Hero.tsx` - Above-the-fold with Spline 3D scene (unloads when out of viewport)
- `components/ChatWidget.tsx` - Chat UI that calls the RAG backend
- `hooks/useRagChat.ts` - React hook for backend API communication

### Performance Patterns

**Smart Loading** (`App.tsx`): Preloads fonts, critical images, and Spline scene (3s timeout fallback) before revealing content.

**Lazy Loading**: All below-the-fold components use React.lazy. Only Hero, Navbar, and ChatWidget are eager-loaded.

**Code Splitting** (`vite.config.ts`): Separate chunks for `vendor-react` and `vendor-3d`.

### Environment Variables

Set in `.env.local`:
- `VITE_RAG_API_URL` - Backend API endpoint (defaults to `http://localhost:7071`)
- `GEMINI_API_KEY` - Optional, accessible via `process.env.API_KEY`

### Styling
- Background: `#0E0F11`
- Text: `#A1A1A6`
- Container max-width: `1600px`

### Types
`types.ts` contains interfaces for NavLink, CaseStudy, Testimonial, ProcessStep.

---

# Part 2: RAG/Agentic Backend

Python Azure Functions backend for AI-powered chat. Currently implements RAG (Retrieval-Augmented Generation) with Azure OpenAI "On Your Data". Designed to be extended into an agentic architecture.

## Development Commands

```bash
cd api
pip install -r requirements.txt
func start                              # Start local function at localhost:7071
func azure functionapp publish <name>   # Deploy to Azure
```

## Infrastructure Deployment

```bash
az deployment group create \
  --resource-group rg-rag-dev \
  --template-file infra/main.bicep \
  --parameters infra/main.bicepparam
```

See `infra/README.md` for full deployment guide and resource details.

## Architecture

### Current Flow (RAG)
```
User Query ‚Üí Azure Function ‚Üí Azure OpenAI "On Your Data" ‚Üí Azure AI Search ‚Üí Response
```

### Tech Stack
- Python 3.11 with Azure Functions v2
- Azure OpenAI (GPT + embeddings)
- Azure AI Search (vector + semantic hybrid search)
- Azure Key Vault for secrets

### API Endpoints (`api/function_app.py`)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/chat` | POST | RAG chat - retrieves from documents |
| `/api/chat-simple` | POST | Direct GPT chat (no retrieval) |
| `/api/health` | GET | Health check |
| `/api/init-index` | POST | One-time search index setup |

### Environment Variables

Set in `api/local.settings.json` locally or Azure portal for production:

| Variable | Description |
|----------|-------------|
| `AZURE_OPENAI_API_KEY` | Azure OpenAI key |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint |
| `AZURE_OPENAI_DEPLOYMENT` | Chat model deployment name |
| `AZURE_EMBEDDING_DEPLOYMENT` | Embedding model deployment name |
| `AZURE_SEARCH_ENDPOINT` | Azure AI Search endpoint |
| `AZURE_SEARCH_KEY` | Azure AI Search admin key |
| `AZURE_SEARCH_INDEX` | Index name (default: `documents-index`) |

## Performance Monitoring & Timing Analysis

The RAG backend includes detailed timing instrumentation to analyze response performance and identify bottlenecks.

### Understanding RAG Parameters

#### `top_n_documents` - How Many Chunks to Retrieve?

Controls how many document chunks are sent to GPT as context.

```python
# api/function_app.py - get_data_source_config()
"top_n_documents": 5  # Default: 5 chunks
```

**How It Works:**
```
User Query: "How does connectome change in Alzheimer's?"
    ‚Üì
Azure AI Search: Vector search finds 50+ relevant chunks
    ‚Üì
Ranked by similarity score: 0.95, 0.92, 0.89, 0.85, 0.82, 0.78...
    ‚Üì
top_n_documents=5 ‚Üí Top 5 chunks selected
    ‚Üì
Sent to GPT: 5 chunks √ó ~800 tokens = ~4000 tokens context
```

**Performance Impact:**

| `top_n_documents` | Context Tokens | Response Time | Accuracy | Cost |
|-------------------|---------------|---------------|----------|------|
| 3 | ~2,400 | **4-5s** | Good ‚úÖ | $ |
| 5 | ~4,000 | **7-8s** | Very Good ‚úÖ‚úÖ | $$ |
| 10 | ~8,000 | **12-15s** | Excessive | $$$ |

**Recommendation:** Use **3** for faster responses, **5** for better accuracy (default).

#### `strictness` - How Strict Should Answers Be?

Controls how tightly GPT adheres to retrieved documents (1-5 scale).

```python
"strictness": 3  # Default: Medium-strict
```

**Strictness Levels:**

| Value | Behavior | Use Case | Example |
|-------|----------|----------|---------|
| **1** | Relaxed - GPT can use own knowledge | General chat | "Tell me about AI" |
| **2** | Balanced - Prefers chunks but flexible | Quick Q&A | "What is connectome?" |
| **3** | Strict - Stays close to chunks | **Academic RAG** ‚úÖ | Current setting |
| **4** | Very strict - Rejects if uncertain | Legal/Medical docs | Contract review |
| **5** | Extreme - Direct quotes only | Compliance | Audit requirements |

**Example Scenario:**

```
Query: "What causes Alzheimer's disease?"

Strictness=1 (Relaxed):
  ‚Üí "Alzheimer's is caused by beta-amyloid plaques..."
     (Uses general knowledge even if not in docs)

Strictness=3 (Default):
  ‚Üí "According to the retrieved documents, Alzheimer's involves..."
     (Cites sources, stays grounded)

Strictness=5 (Extreme):
  ‚Üí "The requested information is not available in the retrieved data."
     (Rejects if exact match not found)
```

**Recommendation:** Keep at **3** for academic RAG. Use **2** for faster, more conversational responses.

---

### Running Timing Analysis

#### Method 1: Direct API Test (Fastest)

Get timing data directly from API response:

```bash
cd /mnt/c/Users/koca/Documents/Coding/website/scripts

curl -s -X POST "https://func-rag-prod-3mktjtlolzx3q.azurewebsites.net/api/chat" \
  -H "Content-Type: application/json" \
  -d '{"message":"Your question here","conversation_history":[]}' \
  | python3 -m json.tool
```

**Response includes timing data:**
```json
{
  "answer": "...",
  "timing": {
    "total_ms": 7725,           // Total response time
    "openai_search_ms": 7717,   // Azure OpenAI + Search time
    "processing_ms": 8          // Response processing time
  },
  "usage": {
    "prompt_tokens": 8085,      // Context size sent to GPT
    "completion_tokens": 404,
    "total_tokens": 8489
  }
}
```

#### Method 2: Application Insights (Detailed Logs)

View detailed logs in Azure Portal:

**Portal Access:**
```
https://portal.azure.com
‚Üí Resource Groups ‚Üí rg-rag-prod
‚Üí ai-rag-prod-3mktjtlolzx3q (Application Insights)
‚Üí Logs
```

**KQL Query for Timing Logs:**
```kusto
traces
| where timestamp > ago(1h)
| where message contains "‚è±Ô∏è" or message contains "üìä"
| project timestamp, message
| order by timestamp desc
| take 50
```

**What You'll See:**
```
‚è±Ô∏è Request validation: 0.003s
‚è±Ô∏è Azure OpenAI + Search: 7.717s
‚è±Ô∏è Response processing: 0.008s
‚è±Ô∏è Total request time: 7.728s | Tokens: 8489
üìä Breakdown - Validation: 0.003s | OpenAI+Search: 7.717s | Processing: 0.008s
```

#### Method 3: Automated Analysis Script

Create a test script for recurring analysis:

```bash
# scripts/analyze_performance.sh
#!/bin/bash

ENDPOINT="https://func-rag-prod-3mktjtlolzx3q.azurewebsites.net/api/chat"
TEST_QUESTION="Alzheimer hastalƒ±ƒüƒ±nda connectome nasƒ±l deƒüi≈üir?"

echo "üß™ Testing RAG Performance..."
curl -s -X POST "$ENDPOINT" \
  -H "Content-Type: application/json" \
  -d "{\"message\":\"$TEST_QUESTION\",\"conversation_history\":[]}" \
  | python3 << 'EOF'
import json, sys

data = json.loads(sys.stdin.read())
t = data['timing']

print(f"\n‚è±Ô∏è  TIMING RESULTS:")
print(f"  Total:          {t['total_ms']:,}ms ({t['total_ms']/1000:.2f}s)")
print(f"  OpenAI+Search:  {t['openai_search_ms']:,}ms ({t['openai_search_ms']/t['total_ms']*100:.1f}%)")
print(f"  Processing:     {t['processing_ms']:,}ms")

if t['total_ms'] < 3000:
    print("\n  üöÄ PERFORMANCE: EXCELLENT")
elif t['total_ms'] < 5000:
    print("\n  ‚úÖ PERFORMANCE: GOOD")
else:
    print("\n  ‚ö†Ô∏è  PERFORMANCE: SLOW - Consider optimizing")

print(f"\nüìä TOKENS: {data['usage']['total_tokens']:,}")
print(f"üìö CITATIONS: {len(data['citations'])} sources\n")
EOF
```

**Run it:**
```bash
chmod +x scripts/analyze_performance.sh
./scripts/analyze_performance.sh
```

---

### Optimizing Performance

If responses are slow (>5 seconds), try these optimizations:

#### 1. Reduce Retrieved Documents

```python
# api/function_app.py - get_data_source_config()
"top_n_documents": 3,  # Changed from 5 ‚Üí saves ~2 seconds
```

#### 2. Lower Strictness

```python
"strictness": 2,  # Changed from 3 ‚Üí faster filtering
```

#### 3. Reduce Max Tokens

```python
# api/function_app.py - chat endpoint
max_tokens=500,  # Changed from 800 ‚Üí faster generation
```

#### 4. Monitor Token Usage

High prompt tokens = slow responses. Check with:
```bash
# Look for high prompt_tokens in response
curl ... | jq '.usage.prompt_tokens'
```

If consistently >8000 tokens, reduce `top_n_documents`.

---

### Performance Benchmarks

**Expected Response Times (Production):**

| Scenario | Config | Time | Quality |
|----------|--------|------|---------|
| Quick answer | top_n=3, strictness=2 | 3-4s | Good |
| **Default (Current)** | **top_n=5, strictness=3** | **6-8s** | **Excellent** |
| Deep research | top_n=10, strictness=4 | 12-15s | Maximum |

**Bottleneck Breakdown (Typical 7.7s response):**
- Azure AI Search query: ~0.5s (6%)
- Azure OpenAI inference: ~7.2s (94%)
- Response processing: <0.01s (<1%)

**Key Insight:** 94% of time is GPT processing large context. To speed up, reduce `top_n_documents`.

---

## Extending to Agentic Architecture

The current RAG system can be evolved into an agentic structure:

### Potential Extensions
- **Tool Use**: Add function calling to let the LLM invoke tools (search, calculate, fetch data)
- **Multi-step Reasoning**: Implement ReAct or similar patterns for complex queries
- **Memory**: Add conversation persistence beyond session
- **Orchestration**: Use LangChain, Semantic Kernel, or custom agent loops
- **Multiple Data Sources**: Extend beyond document search to APIs, databases, web

### Suggested Structure for Agents
```
api/
‚îú‚îÄ‚îÄ function_app.py      # HTTP endpoints (keep thin)
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py  # Main agent loop
‚îÇ   ‚îú‚îÄ‚îÄ tools/           # Tool implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calculator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ web.py
‚îÇ   ‚îî‚îÄ‚îÄ memory/          # Conversation/context management
‚îú‚îÄ‚îÄ prompts/             # System prompts and templates
‚îî‚îÄ‚îÄ requirements.txt
```

### Key Considerations
- Keep `function_app.py` as a thin HTTP layer
- Implement agent logic in separate modules
- Use structured outputs for tool calls
- Add observability (logging, tracing) for debugging agent behavior

---

# Security

## Implemented Protections

### Backend (`api/security.py`)
- **Rate Limiting**: 10 requests/minute for chat, 20 for general endpoints
- **IP Blocking**: Automatic 10-minute block after 5 rate limit violations
- **CORS Whitelisting**: Only allows requests from configured origins
- **Request Validation**: Input sanitization, length limits, type checking
- **HMAC Signing**: Optional request signing for admin endpoints
- **Security Headers**: X-Content-Type-Options, X-Frame-Options, X-XSS-Protection

### Infrastructure (`infra/main.bicep`)
- **TLS 1.2 minimum** on all services
- **HTTPS only** on Function App
- **FTPS disabled**
- **Key Vault** for all secrets with RBAC authorization
- **Purge protection** enabled on Key Vault
- **Application Insights** for monitoring
- **Managed Identity** enabled on Function App

### Authentication (Managed Identity)
- **Azure OpenAI**: Uses Managed Identity with `DefaultAzureCredential`
- **Azure AI Search**: Uses Managed Identity for production, API key for local dev
- **Key Vault**: Automatic access via Managed Identity RBAC
- **Zero Secrets**: No hardcoded API keys in production environment

```python
# api/function_app.py - Managed Identity Implementation
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

def get_openai_client() -> AzureOpenAI:
    api_key = os.environ.get("AZURE_OPENAI_API_KEY", "")

    if api_key and not api_key.startswith("@Microsoft.KeyVault"):
        # Local development with API key
        return AzureOpenAI(api_key=api_key, ...)
    else:
        # Production: Use Managed Identity
        credential = DefaultAzureCredential()
        token_provider = get_bearer_token_provider(
            credential, "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(azure_ad_token_provider=token_provider, ...)
```

### Frontend
- Rate limit awareness with automatic retry handling
- Proper error messages for security-related failures
- No secrets in frontend code or environment variables

## Environment Variables (Security)

| Variable | Description |
|----------|-------------|
| `ALLOWED_ORIGINS` | Comma-separated list of allowed CORS origins |
| `RATE_LIMIT_WINDOW` | Rate limit window in seconds (default: 60) |
| `RATE_LIMIT_MAX_REQUESTS` | Max requests per window (default: 20) |
| `RATE_LIMIT_CHAT_MAX` | Max chat requests per window (default: 10) |
| `API_SECRET_KEY` | HMAC secret for request signing (optional) |

## Rotating Compromised Keys

If keys are exposed, rotate immediately:

```bash
# Azure OpenAI - regenerate in Azure Portal
az cognitiveservices account keys regenerate \
  --name <openai-resource> \
  --resource-group <rg> \
  --key-name key1

# Azure Search - regenerate admin key
az search admin-key renew \
  --service-name <search-name> \
  --resource-group <rg> \
  --key-type primary

# Update Key Vault secrets after regeneration
az keyvault secret set \
  --vault-name <vault> \
  --name azure-openai-key \
  --value "<new-key>"
```

## Files to Never Commit

See `.gitignore` - critical files:
- `api/local.settings.json` (contains secrets for local dev)
- `.env`, `.env.local` (environment variables)
- `scripts/.env` (indexing script secrets)
- Any `*.key` or `*.pem` files

---

# Document Intelligence & Semantic Chunking

## Overview

This project uses Azure Document Intelligence (Form Recognizer) for PDF text extraction and LangChain for semantic chunking. This is a **two-stage offline process** for indexing academic papers.

## Architecture

```
PDF Documents (scripts/data/)
    ‚Üì
Azure Document Intelligence (prebuilt-read OCR)
    ‚Üì
Full Text Extraction
    ‚Üì
LangChain RecursiveCharacterTextSplitter
    ‚Üì
Semantic Chunks (~750-800 tokens each)
    ‚Üì
Azure OpenAI Embeddings (text-embedding-3-large)
    ‚Üì
Azure AI Search Index (vector + metadata)
```

## Why Semantic Chunking?

### ‚ùå Old Approach (Page-Based)
```
Page 1: Introduction + Methods beginning
Page 2: Methods end + Results beginning
‚Üí Context lost, GPT confused
```

### ‚úÖ New Approach (Semantic)
```
Chunk 1: Introduction (complete section)
Chunk 2: Methods - Data Collection
Chunk 3: Methods - Analysis
‚Üí Each chunk is meaningful, GPT accurate
```

## Semantic Chunking Configuration

```python
# scripts/index_documents.py
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # ~750-800 tokens
    chunk_overlap=200,          # Context preservation
    length_function=lambda t: len(tiktoken.encode(t)),  # Token-based
    separators=[
        "\n\n",                 # Paragraph (highest priority)
        "\n",                   # Line
        ". ",                   # Sentence
        " ",                    # Word
        ""                      # Character (fallback)
    ]
)
```

### Key Parameters:
- **chunk_size**: 1000 characters (~750-800 tokens for GPT-4o)
- **chunk_overlap**: 200 characters (ensures context not lost at boundaries)
- **Token counting**: Uses `tiktoken` for accurate token estimation
- **Separator hierarchy**: Tries to split on paragraphs first, then lines, sentences, etc.

## Document Intelligence Usage

**Model**: `prebuilt-read` (OCR for digital & scanned PDFs)

**Why prebuilt-read?**
- ‚úÖ Academic papers are mostly text
- ‚úÖ Works on both digital and scanned PDFs
- ‚úÖ Fast and cost-effective (~$1.50 per 1000 pages)
- ‚ùå No need for `prebuilt-layout` (tables not critical for RAG)

```python
# Extract text from PDF
poller = doc_client.begin_analyze_document("prebuilt-read", document=file)
result = poller.result()

# Combine all pages
full_text = ""
for page in result.pages:
    page_text = " ".join([line.content for line in page.lines])
    full_text += page_text + "\n\n"
```

## Indexing Workflow

### Step 1: Setup Environment
```bash
cd scripts
cp .env.example .env
# Fill in:
# - AZURE_FORM_RECOGNIZER_ENDPOINT
# - AZURE_FORM_RECOGNIZER_KEY
# - AZURE_OPENAI_API_KEY
# - AZURE_SEARCH_KEY
```

### Step 2: Add PDFs
```bash
cp my_academic_paper.pdf data/
```

### Step 3: Run Indexing
```bash
python index_documents.py
```

**Output:**
```
============================================================
üìö ƒ∞≈üleniyor: paper.pdf
============================================================
üìÑ Okunuyor: data/paper.pdf...
   ‚úÖ 15 sayfa okundu, toplam 45231 karakter.
   üß© 12 semantic chunk olu≈üturuldu (avg ~3769 char/chunk)
   üîÑ Embedding'ler olu≈üturuluyor...
   ‚úÖ Chunk 1/12 | Page 1 | 782 tokens
   ‚úÖ Chunk 2/12 | Page 2 | 795 tokens
   ...
============================================================
üöÄ 12 chunk Azure AI Search'e y√ºkleniyor...
============================================================
   üì¶ Batch 1: 12 chunk y√ºklendi

‚úÖ T√ºm d√∂k√ºmanlar ba≈üarƒ±yla indexlendi!
   üìä Toplam: 12 semantic chunk
```

## Chunk Structure

Each chunk uploaded to Azure AI Search contains:

```python
{
    "id": "paper_pdf-chunk1",              # Unique ID
    "content": "Full text of the chunk...", # Actual content
    "title": "paper.pdf",                  # Source document
    "source": "paper.pdf",                 # Source file
    "chunk_id": 1,                         # Chunk number
    "content_vector": [0.123, ...]         # 3072-dim embedding
}
```

## Performance Comparison

| Metric | Page-Based | Semantic |
|--------|-----------|----------|
| Avg Chunk Size | 500-2000 tokens (variable) | 750-800 tokens (stable) |
| Context Preservation | ‚ùå Poor | ‚úÖ Excellent |
| RAG Answer Quality | 6/10 | 9/10 |
| Citation Accuracy | 5/10 | 9/10 |

## Cost Estimation

**For 10 academic papers (~150 pages total):**
- Document Intelligence: ~$0.22 (150 pages √ó $1.50/1000)
- OpenAI Embeddings: ~$0.15 (assuming 120K tokens)
- **Total**: ~$0.37 per indexing run

## Troubleshooting

**Issue**: "Module not found: tiktoken"
```bash
cd scripts
pip install -r requirements.txt
```

**Issue**: "Rate limit exceeded"
- Script has `time.sleep(0.3)` between embeddings
- If still failing, increase to `time.sleep(0.5)`

**Issue**: Token count too high
- Reduce `chunk_size=800` instead of 1000
- Check with: `len(tiktoken.encode(chunk))`

## Updating Chunking Strategy

If you need to change chunking parameters:

1. Update `scripts/index_documents.py`:
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # Smaller chunks
    chunk_overlap=150,
    ...
)
```

2. **Delete old index** (chunks will have different structure):
```bash
az search index delete \
  --name documents-index \
  --service-name search-rag-prod-3mktjtlo
```

3. **Re-index all documents**:
```bash
cd scripts
python index_documents.py
```

---

# GIT WORKFLOW for Claude Code

## üìù When to Commit and Push

### Always Commit When:
1. ‚úÖ Completed a logical unit of work (feature, fix, refactor)
2. ‚úÖ Added new functionality that works
3. ‚úÖ Fixed a bug successfully
4. ‚úÖ Updated documentation
5. ‚úÖ Made security improvements
6. ‚úÖ User explicitly requests "push changes" or "commit this"

### Never Commit When:
1. ‚ùå Code is broken or untested
2. ‚ùå Contains secrets/API keys
3. ‚ùå Work is incomplete (unless user specifically asks)
4. ‚ùå You're just exploring/reading code

## üîÑ Standard Git Workflow

### Step 1: Check Status
```bash
git status
```
Review what files changed. Ensure no secrets are staged.

### Step 2: Stage Changes
```bash
# Stage specific files
git add api/function_app.py api/requirements.txt

# Or stage all (be careful!)
git add .

# Verify what's staged
git diff --staged
```

### Step 3: Commit with Descriptive Message
```bash
git commit -m "type: Brief description

- Detailed change 1
- Detailed change 2
- Detailed change 3

ü§ñ Generated with Claude Code
Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

**Commit Types:**
- `feat:` New feature
- `fix:` Bug fix
- `refactor:` Code restructuring
- `docs:` Documentation changes
- `security:` Security improvements
- `perf:` Performance improvements
- `chore:` Maintenance tasks

### Step 4: Push to GitHub
```bash
# Push to main branch
git push origin main

# Or if on feature branch
git push origin feature/branch-name
```

### Step 5: Verify
```bash
git log --oneline -3
```

## üìã Example Commit Session

```bash
# Scenario: Just added Managed Identity and semantic chunking

# 1. Check what changed
git status

# 2. Review changes
git diff api/function_app.py
git diff scripts/index_documents.py

# 3. Stage files
git add api/function_app.py
git add api/requirements.txt
git add scripts/index_documents.py
git add scripts/requirements.txt
git add scripts/README.md
git add scripts/.env.example
git add README.md
git add CLAUDE.md

# 4. Commit with message
git commit -m "feat: Add Managed Identity and semantic chunking

Backend improvements:
- Implement Managed Identity for Azure OpenAI and AI Search
- Remove hardcoded API keys, use Key Vault references
- Add DefaultAzureCredential with fallback to API keys

Document indexing improvements:
- Implement semantic chunking with RecursiveCharacterTextSplitter
- Token-based chunking (~750-800 tokens per chunk)
- Add 200-token overlap for context preservation
- Create detailed indexing script documentation

Security:
- Azure services now authenticate via Managed Identity
- API keys stored in Key Vault (production)
- Zero exposed secrets in production environment

ü§ñ Generated with Claude Code
Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

# 5. Push to GitHub
git push origin main

# 6. Verify
git log --oneline -3
```

## üö® Important Git Rules

### Rule 1: Never Force Push to Main
```bash
# ‚ùå NEVER do this
git push --force origin main

# ‚úÖ If you need to fix a commit, create a new one
git commit --amend  # Only for unpushed commits
```

### Rule 2: Check for Secrets Before Committing
```bash
# Always verify no secrets are being committed
git diff --staged | grep -i "key\|secret\|password\|token"

# If found, unstage and add to .gitignore
git reset HEAD <file>
```

### Rule 3: Pull Before Push (If Working with Others)
```bash
git pull origin main
git push origin main
```

### Rule 4: Use Branches for Major Features
```bash
# Create feature branch
git checkout -b feature/new-feature

# Work on feature
git add .
git commit -m "feat: Implement new feature"

# Push branch
git push origin feature/new-feature

# Create PR via GitHub UI or gh CLI
gh pr create --title "feat: New feature" --body "Description..."
```

## üîç Useful Git Commands for Claude Code

```bash
# See what changed in working directory
git diff

# See what's staged for commit
git diff --staged

# See commit history
git log --oneline -10

# See specific file history
git log --oneline -- api/function_app.py

# Undo staged changes (before commit)
git reset HEAD <file>

# Undo local changes (DANGEROUS - loses work)
git checkout -- <file>

# See current branch
git branch

# See remote URL
git remote -v
```

## üì§ Deployment After Push

### Frontend (Automatic)
```
git push ‚Üí GitHub Actions ‚Üí Azure Static Web Apps
```
- Automatic deployment on every push to `main`
- No manual action needed
- Check: https://mertoshi.online

### Backend (Manual)
After pushing backend changes, deploy Function App:

```bash
cd api
func azure functionapp publish func-rag-prod-3mktjtlolzx3q

# Verify deployment
curl https://func-rag-prod-3mktjtlolzx3q.azurewebsites.net/api/health
```

Or use GitHub Actions:
```bash
gh workflow run deploy-rag-infra.yml
```

---
